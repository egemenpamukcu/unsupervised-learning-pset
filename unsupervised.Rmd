---
title: "Homework 4: Unsupervised Learning"
subtitle: |
  | MACS 30100: Perspectives on Computational Modeling
  | University of Chicago
output: pdf_document
urlcolor: blue
---

## Overview

For each of the following prompts, produce responses _with_ code in-line. While you are encouraged to stage and draft your problem set solutions using any files, code, and data you'd like within the private repo for the assignment, *only the final, rendered PDF with responses and code in-line will be graded.*

Note: take a look at the `hw04.pdf` file to see a better rendering of this problem set (e.g., cleaner looking table, etc.). 

## Dimension Reduction

### Conceptual Problems

1. (5 points) Compute the total variance from the following PCA output.

|  | PC1 | PC2 | PC3 | PC4 | PC5 | PC6 | PC7 | PC8 | PC9 | PC10 |
|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|
| Standard deviation | 3.55 | 2.41 | 1.82 | 1.31 | 1.05 | 0.86 | 0.81 | 0.79 | 0.72 | 0.70 |
| Variance | 3.45 | 3.10 | 1.75 | 0.98 | 0.64 | 0.33 | 0.31 | 0.30 | 0.09 | 0.05 |

```{r}
library(tidyverse)
library(corrr)
library(amerika)
library(factoextra)
library(patchwork)
library(ggrepel)

var <- c(3.45, 3.10, 1.75, 0.98, 0.64, 0.33, 0.31, 0.3, 0.09, 0.05)
tot_var <- sum(var)
tot_var
```

2. (10 points) Make a *manual* scree plot based on these results. That is, *no* canned functions or packages (e.g., `factoextra`).
```{r}
scree <- tibble(PC = 1:10, PVE = var / tot_var)
ggplot(data = scree) + 
  geom_line(aes(x = PC, y = PVE)) +
  geom_point(aes(x = PC, y = PVE))
```


3. (10 points) Based on your results in the previous question, how many PCs would you suggest characterize these data well? That is, what would the dimensionality of your new reduced data space be?

```{r}
add <- 0
cumPVE <- rep(0, length(scree$PVE))
for (i in 1:length(scree$PVE)) {
  cumPVE[i] <- add + scree$PVE[i]
  add = cumPVE[i]
}
scree$cPVE <- cumPVE

ggplot(data = scree) + 
  geom_line(aes(x = PC, y = cPVE)) +
  geom_point(aes(x = PC, y = cPVE))

scree
```
Three principal components seems to explain most of the variation in the data set with 75 while also allowing visualization of the data in three dimensional space. Looking at the scree plot, 


4. (10 points) Calculate the Euclidean distance between each of the following observations, $i$, and some observation at 0 (i.e., $x_0$) in 4-dimensional space $\forall X \in \{1,2,3,4\}$.

| $i$ | $X_1$ | $X_2$ | $X_3$ | $X_4$ | Euclidean Distance
|:----:|:----:|:----:|:----:|:----:|:----:|
| 1 | 2 | 2 | 3 | 1 | $\dots$ |
| 2 | 1 | 1 | -2 | 2 | $\dots$ |
| 3 | 1 | -2 | -2 | -1 | $\dots$ |
| 4 | 3 | 3 | 2 | 2 | $\dots$ |
| 5 | -3 | 2 | -1 | 1 | $\dots$ |

```{r}
x1 <- c(2, 2, 3, 1)
x2 <- c(1, 1, -2, 2)
x3 <- c(1, -2, -2, -1)
x4 <- c(3, 3, 2, 2)
x5 <- c(-3, 2, -1, 1)
vectors <- list(x1, x2, x3, x4, x5)

distances <- rep(0, 5)
for (i in 1:length(vectors)) {
   distances[i] <- sqrt(sum(vectors[[i]]^2))
}

data.frame(i = seq(1, 5), distance = distances)
```


### An Applied Problem

For the following applied problem, use the 2019 American National Election Study (ANES) Pilot survey data. These data include, among many other features, a battery of 35 feeling thermometers, which are questions with answers ranging from 1 to 100 for how respondents "rate" some topic (e.g., *How would you rate Obama?* or *How would you rate Japan?*). See the documentation and more detail [here.](https://electionstudies.org/data-center/2019-pilot-study/) 

To make your lives a bit easier, I have preprocessed the data for you, including: 1) feature engineering (via kNN) for missing data, and 2) reduction of the feature space to include only the 35 feeling thermometers and a feature for the respondent's party affiliation (`democrat`), where 1 = Democrat and 0 = non-Democrat (which could be Republican, Independent, or decline to say).

5. (10 points) Fit a PCA model on all 35 feeling thermometers from the 2019 ANES, but be careful to *not* include the party affiliation feature.

```{r}
anes <- read_rds('/Users/egemenpamukcu/Downloads/ps4-egemenpamukcu-main/data/anes.rds')

pca_fit <- anes[,-36] %>%
  scale() %>% 
  prcomp()

summary(pca_fit)
```


6. (20 points) Plot the feature contributions from each of the feeling thermometers in the first two dimensions (i.e., PC1 and PC2). Describe the patterns, groupings, and structure of the lower-dimensional projections in *substantive* terms.
```{r}

pca_fit %>% 
  fviz_pca_biplot(label = "var",
                  col.var = amerika_palettes$Republican[2],
                  col.ind = amerika_palettes$Democrat[3]) +
  labs(title = "") +
  theme_minimal()

```
It seems like the (upper) right and (lower) left corners of the two dimensional representation of our data corresponds to Democrats and Republicans respectively. The observations accumulated near the upper right corner of the biplot have warmer feelings towards politicians affiliated with the Democratic Party, such as Obama, Biden, and Harris. They also happen to be more 'globalist' as they have warmer feelings towards the United Nations and illegal immigrants. The lower left corner corresponds to three features that is positively correlated with Republican party identity, namely Trump, ICE and NSA. The loading vectors for ICE and Illegal Immigrant features are nearly 180 degrees apart, which indicates that, expectedly, these two features are negatively correlated, same logic applies to the Trump and Obama feature pair as well. The lower right corner of the visualization houses relatively more bipartisan features, as most of them are almost orthogonal to the directly partisan features. Nevertheless, some party identification can still be inferred as the features "White", "Israel" and "Russia" are closer to Republican Party identity, whereas "Palestine" and "Muslim" are closer to features affiliated with Democratic Party. 

## Clustering

### A Conceptual Problem

7. (10 points) What are the two properties required for a *hard* partitional solution, and when thus relaxed, give a *soft* partitional clustering solution? Be sure to answer this both formally (with mathematical notation) and substantively (with words). Then, give an example or two of each and how they relate to these two central properties of clustering. 

Hard partitioning methods such as k-means assign each observation to the cluster it 'fits' the best. At the end of the clustering process, each observation belongs to one of the kth clusters so that $c_1 \cup c_2 \cup  ... c_k$ covers the all the observations in the data set. Hard clustering also does not allow overlapping between clusters which can be shown as $C_k \cap c'_k = \varnothing$. Soft partitional clustering instead allows overlapping clusters and, instead of assigning each observation to the most likely cluster, it returns a probability distribution indicating the likelihood of an observation belonging to a specific clusters. An example of hard partitioning is k-means where you decide beforehand how many clusters you want (k) and the alogrithm will classify every observation as belonging to one of those groups. An example of soft partitioning is GMM which uses distribution assumptions to weight the probabilities of certain observations belonging to clusters.

### An Applied Problem

In this applied problem, you will again use the 2019 ANES data, but this time to explore the clustering solution from fitting a fuzzy c-means (FCM) algorithm to all feeling thermometers. As with the dimension reduction exercise, derive a clustering solution using *only* the feeling thermometers. The idea here is to explore whether attitudes on these issues, countries, and people map onto natural groupings between major American political parties.

8. (5 points) Load and scale the ANES *feeling thermometer* data.
```{r}
anes_scaled <- anes %>% 
  scale()
anes_scaled <- as.tibble(anes_scaled)
anes_scaled$democrat <- anes$democrat
anes_scaled
```


9. (5 points) Fit an FCM algorithm to the scaled data initialized at $k = 2$, driven by the assumption that party affiliation (Democrat or non-Democrat) underlies these data.
```{r}
library(e1071)
anes_scaled <- anes_scaled[, -36]

fuzzy <- cmeans(x = anes_scaled, centers = 2)
anes_scaled$cluster <- fuzzy$cluster
```


10. (15 points) Visualize the cluster scores from your FCM solution plotted over the range of feelings toward `Trump` and `Obama`, with data points colored by cluster assignment and also labeled by the respondent's true party affiliation (the `democrat` feature). As party wasn't included in your clustering solution, what can you conclude based on these patterns? Is there a grouping pattern among observations along a partisan dimension, or isn't there? Do respondents group in expected ways (e.g., Trump supporters to the right and Obama supporters to the left)? Do cluster assignments align with the true party affiliation or not? How would you evaluate the effectiveness of FCM for this type of task?

```{r}
anes_scaled$democrat = anes$democrat

ggplot(data = anes_scaled) +
  geom_text(aes(x = Trump, y = Obama, label=democrat, col = as.factor(cluster)))
  
```
Looking at the visualization, the fuzzy c-means clustering method seems to capture most of the party affiliation in the dataset. Roughly the party identification in this two dimensional representation corresponds to the lower right and upper left corners (people who hate and love either Trump or Obama) and the coloring in the graph seems to match that pattern. Most of the 0s (non-Democrats) are colored in blue and most of the democrats (1s) are colored in red. As expected, for the observations laying close to the lower left - upper right diagonal, the clustering results align less with the true party identification which indicates the effect of the feeling thermometers for Trump and Obama on the clustering predictions. In general, FCM does a good job in understanding party identity in an unsupervised manner. 
